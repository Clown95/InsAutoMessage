package transaction

import (
	"InsAutoMessage/config"
	DB "InsAutoMessage/database"
	"InsAutoMessage/utils"
	"context"
	"encoding/base64"
	"encoding/json"
	"fmt"
	"github.com/chromedp/cdproto/network"
	"github.com/chromedp/chromedp"
	"io"
	"log"
	"math/rand"
	"net/http"
	"net/url"
	"strings"
	"sync"
	"time"
)

var (
	SearchLocationsMap sync.Map
)

// Response å®šä¹‰ç»“æ„ä½“è§£æ GraphQL è¿”å›çš„ JSON æ•°æ®
type Response struct {
	Data struct {
		XDTLocationGetWebInfoTab struct {
			Edges []struct {
				Node struct {
					User struct {
						Username string `json:"username"`
					} `json:"user"`
					Owner struct {
						Username string `json:"username"`
					} `json:"owner"`
					CoauthorProducers []struct {
						Username string `json:"username"`
					} `json:"coauthor_producers"`
					Usertags struct {
						In []struct {
							User struct {
								Username string `json:"username"`
							} `json:"user"`
						} `json:"in"`
					} `json:"usertags"`
				} `json:"node"`
			} `json:"edges"`
			PageInfo struct {
				EndCursor   string `json:"end_cursor"`
				HasNextPage bool   `json:"has_next_page"`
			} `json:"page_info"`
		} `json:"xdt_location_get_web_info_tab"`
	} `json:"data"`
}

func EnBase64(base64String string) string {
	decodedBytes, err := base64.StdEncoding.DecodeString(base64String)
	if err != nil {
		log.Println("è§£ç é”™è¯¯:", err)
		return ""
	}
	return string(decodedBytes)
}

// extractUsernames ä»è¿”å›æ•°æ®ä¸­æå–æ‰€æœ‰ä¸é‡å¤çš„ usernameï¼Œå¹¶è¿”å›æ˜¯å¦è¿˜æœ‰ä¸‹ä¸€é¡µ
func extractUsernames(response Response) ([]string, bool) {
	usernameSet := make(map[string]bool)
	var usernames []string

	// ä» user å­—æ®µæå–
	for _, edge := range response.Data.XDTLocationGetWebInfoTab.Edges {
		usernameSet[edge.Node.User.Username] = true
	}
	// ä» owner å­—æ®µæå–
	for _, edge := range response.Data.XDTLocationGetWebInfoTab.Edges {
		usernameSet[edge.Node.Owner.Username] = true
	}
	// ä» coauthor_producers å­—æ®µæå–
	for _, edge := range response.Data.XDTLocationGetWebInfoTab.Edges {
		for _, producer := range edge.Node.CoauthorProducers {
			usernameSet[producer.Username] = true
		}
	}
	// ä» usertags å­—æ®µæå–
	for _, edge := range response.Data.XDTLocationGetWebInfoTab.Edges {
		for _, tag := range edge.Node.Usertags.In {
			usernameSet[tag.User.Username] = true
		}
	}
	// å°† map ä¸­çš„é”®è½¬æ¢ä¸ºåˆ‡ç‰‡
	for username := range usernameSet {
		usernames = append(usernames, username)
	}

	hasNextPage := response.Data.XDTLocationGetWebInfoTab.PageInfo.HasNextPage

	return usernames, hasNextPage
}

// sendHTTPRequest é€šè¿‡ä»£ç†å‘é€ POST è¯·æ±‚è·å–æ¥å£å“åº”å†…å®¹
func sendHTTPRequest(proxyArr, payloads, cookieStr string) (string, error) {
	url1 := "https://www.instagram.com/graphql/query"
	method := "POST"

	payload := strings.NewReader(payloads)
	proxy, err := url.Parse(proxyArr)
	if err != nil {
		log.Println("è§£æä»£ç†åœ°å€é”™è¯¯:", err)
		return "", err
	}

	// é…ç½®ç½‘ç»œä¼ è¾“
	netTransport := &http.Transport{
		Proxy:                 http.ProxyURL(proxy),
		MaxIdleConnsPerHost:   10,
		ResponseHeaderTimeout: 5 * time.Second,
	}
	// åˆ›å»º HTTP å®¢æˆ·ç«¯
	client := &http.Client{
		Timeout:   10 * time.Second,
		Transport: netTransport,
	}
	req, err := http.NewRequest(method, url1, payload)
	if err != nil {
		log.Println(err)
		return "", err
	}
	req.Header.Add("Content-Type", "application/x-www-form-urlencoded")
	req.Header.Add("User-Agent", "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "+
		"AppleWebKit/537.36 (KHTML, like Gecko) Chrome/133.0.0.0 Safari/537.36")
	req.Header.Add("X-IG-App-ID", "936619743392459")
	req.Header.Add("COOKIE", cookieStr)

	res, err := client.Do(req)
	if err != nil {
		log.Println("HTTPè¯·æ±‚å¤±è´¥:", err)
		return "", err
	}
	defer res.Body.Close()

	body, err := io.ReadAll(res.Body)
	if err != nil {
		log.Println("è¯»å–å“åº”ä½“å¤±è´¥:", err)
		return "", err
	}
	return string(body), nil
}

// extractAndProcessPayload æå–å¹¶å¤„ç† PostDataEntries
func extractAndProcessPayload(entries []*network.PostDataEntry) string {
	var payload string
	for _, entry := range entries {
		payload = EnBase64(entry.Bytes)
	}
	return payload
}

// SearchJobApi é€šè¿‡æµè§ˆå™¨å®ä¾‹å’Œ API è¯·æ±‚é‡‡é›†æŒ‡å®šåœ°ç‚¹çš„åšä¸»æ•°æ®
func SearchJobApi(workerID int, accountFile, locationId string) error {
	log.Printf("å¯åŠ¨æµè§ˆå™¨å®ä¾‹å¤„ç†åœ°ç‚¹: %s", locationId)

	// åˆ›å»ºå¸¦è¶…æ—¶æ§åˆ¶çš„ ExecAllocator å’Œä¸Šä¸‹æ–‡ï¼ˆä¾‹å¦‚3åˆ†é’Ÿè¶…æ—¶ï¼‰
	opts := utils.SetChromeOptionsImgless()
	allocCtx, cancelAlloc := chromedp.NewExecAllocator(context.Background(), opts...)
	defer cancelAlloc()

	ctx, cancelCtx := chromedp.NewContext(allocCtx)
	// æ­¤å¤„è®¾ç½®è¶…æ—¶ï¼Œé˜²æ­¢é¡µé¢æ“ä½œæ— é™ç­‰å¾…
	ctx, cancelTimeout := context.WithTimeout(ctx, 2*time.Minute)

	// ç»Ÿä¸€ defer ä¿è¯å–æ¶ˆé¡ºåº
	defer func() {
		cancelTimeout()
		cancelCtx()
		cancelAlloc()
	}()

	// åŠ è½½ Cookies æ–‡ä»¶
	if err := utils.LoadCookies(ctx, accountFile); err != nil {
		log.Printf("åŠ è½½Cookieså¤±è´¥: %v", err)

		return fmt.Errorf("åŠ è½½Cookieså¤±è´¥: %w", err)

	}

	var payloadStr string
	// ç›‘å¬ç½‘ç»œè¯·æ±‚ï¼Œæ•è·ç›®æ ‡æ¥å£çš„è¯·æ±‚è½½è·
	chromedp.ListenTarget(ctx, func(ev interface{}) {
		if ev, ok := ev.(*network.EventRequestWillBeSent); ok {
			if ev.Request.URL == "https://www.instagram.com/graphql/query" {
				if ev.Request.HasPostData {
					postData := extractAndProcessPayload(ev.Request.PostDataEntries)

					if strings.Contains(postData, "variables=%7B%22after") && !strings.Contains(postData, "after%22%3Anull") {
						//postData = strings.ReplaceAll(postData, "ranked", "recent")

						payloadStr = postData

						//payloadStr = strings.ReplaceAll(payloadStr, "ranked", "recent")

						log.Printf("ğŸ”¹ è¯·æ±‚è½½è·:%s \n", payloadStr)

					}
				}
			}
		}
	})

	// å¯¼èˆªè‡³æŒ‡å®šåœ°ç‚¹é¡µé¢ï¼Œå¹¶è§¦å‘æ»šåŠ¨æ“ä½œ
	if err := chromedp.Run(ctx,
		chromedp.Navigate(fmt.Sprintf("https://www.instagram.com/explore/locations/%s", locationId)),
		chromedp.Sleep(2*time.Second),
		chromedp.Evaluate(`window.scrollTo(0, document.body.scrollHeight)`, nil),
		chromedp.Sleep(3*time.Second),
	); err != nil {
		log.Printf("æµè§ˆå™¨æ“ä½œå¤±è´¥: %v", err)
		return fmt.Errorf("æµè§ˆå™¨æ“ä½œå¤±è´¥: %w", err)
	}

	body := ""
	Url := ""
	err := chromedp.Run(ctx,
		chromedp.WaitReady(`img`, chromedp.ByQuery),
		chromedp.Text(`body`, &body, chromedp.ByQuery),
		chromedp.Location(&Url),
	)
	if err != nil {
		return err
	}

	go func() {
		ticker := time.NewTicker(800)
		defer ticker.Stop()

		timeout := time.After(60 * time.Second)
		log.Println("æ­£åœ¨æ£€æµ‹")

		for {
			//
			select {
			case <-ticker.C:
				err = CheckAlreadyLoggedStatus(ctx, accountFile)
				if err != nil {
					cancelCtx()
					return
				}
			case <-timeout:
				cancelCtx()
				return
			}
		}

	}()

	if strings.Contains(body, "å‡ºç°é—®é¢˜ï¼Œé¡µé¢æ— æ³•åŠ è½½ã€‚") {
		log.Println("å½“å‰åœ°å€é”™è¯¯")

		err := DB.UpdateLocationState(config.GormDb, locationId, 1)
		if err != nil {
			return err
		}
		return fmt.Errorf("å½“å‰åœ°å€é”™è¯¯")
	}

	// è·å–å¸–å­æ•°é‡
	log.Println("è·å–å¸–å­æ•°é‡")
	// è·å–å¸–å­æ•°é‡
	count, err := GetPostStatistics(ctx)
	if err != nil || count == 0 {

		chromedp.Cancel(ctx)
		return fmt.Errorf("è·å–å¸–å­æ•°é‡å¤±è´¥: %w", err)
	}

	/*
		chromedp.Run(ctx,
			chromedp.WaitReady(`//span[text()='æœ€æ–°']`, chromedp.BySearch),
			chromedp.Click(`//span[text()='æœ€æ–°']`, chromedp.BySearch),
		)

	*/

	// æ›´æ–°æ•°æ®åº“ä¸­çš„å¸–å­æ•°é‡
	loc, err := DB.GetLocationByID(config.GormDb, locationId)
	if err != nil {
		log.Println("æŸ¥è¯¢å¤±è´¥:", err)
	} else {
		//log.Printf("æŸ¥è¯¢ç»“æœ: %+v\n", loc)
	}
	loc.Count = count
	if err := DB.UpdateLocation(config.GormDb, loc); err != nil {
		log.Println("æ›´æ–°å¤±è´¥:", err)
	}

	// å¦‚æœ payloadStr ä¸ºç©ºï¼Œåˆ™å°è¯• Reload å¹¶ç­‰å¾…ä¸€æ®µæ—¶é—´æ•è·
	if payloadStr == "" {
		if err := chromedp.Run(ctx,
			//network.Enable(),
			chromedp.Reload(),
			chromedp.Sleep(2*time.Second),
		); err != nil {
			log.Printf("Reloadå¤±è´¥: %v", err)
		}
		// ç­‰å¾…æœ€å¤š10ç§’çœ‹æ˜¯å¦æ•è·åˆ° payload
		deadline := time.Now().Add(10 * time.Second)
		for payloadStr == "" && time.Now().Before(deadline) {
			time.Sleep(500 * time.Millisecond)
		}
		if payloadStr == "" {
			log.Printf("[Worker %d] é‡è¯•åä»æœªæ•è·åˆ° payload (location=%s)", workerID, locationId)

			return fmt.Errorf("æœªæ•è·åˆ°æœ‰æ•ˆçš„ payload (location=%s)", locationId)
		}
	}

	cookiesStr, err := utils.ToCookieStr(accountFile)
	if err != nil && cookiesStr == "" {
		return fmt.Errorf("cookiesè½¬å­—ç¬¦ä¸²å¤±è´¥: %w", err)
	}

	currentCount := 0
	for {

		body, err = sendHTTPRequest(config.AppCfg.Proxyaddr, payloadStr, cookiesStr)
		if err != nil {

			return fmt.Errorf("è·å–æ•°æ®å¤±è´¥: %w", err)
		}
		var response Response
		if err := json.Unmarshal([]byte(body), &response); err != nil {
			//return fmt.Errorf("è§£æJSONå¤±è´¥: %w", err)

			return fmt.Errorf("è§£æJSONå¤±è´¥: %w", err)

		}

		if len(response.Data.XDTLocationGetWebInfoTab.Edges) == 0 {

			return fmt.Errorf("æœªæ‰¾åˆ°æ•°æ®")
		}

		// æå–ç”¨æˆ·åå’Œæ˜¯å¦è¿˜æœ‰ä¸‹ä¸€é¡µæ•°æ®
		usernames, hasNextPage := extractUsernames(response)
		var bloggersTmp []*DB.IndiaBlogger

		for _, nickName := range usernames {
			blogHome := fmt.Sprintf("https://www.instagram.com/%s/", url.PathEscape(nickName))
			blogger := &DB.IndiaBlogger{
				Nickname: nickName,
				Bloghome: blogHome,
				Issend:   0,
				Isreply:  0,
			}
			/*
				if err := DB.CreateBlogger(config.GormDb, blogger); err != nil {
					if !DB.IsDuplicateError(err) {
						log.Printf("æ’å…¥å¤±è´¥: %s - %v", nickName, err)
					}
					continue
				}
			*/
			log.Printf("[Worker %d]  é‡‡é›†åˆ°: %s ï¼Œå·²é‡‡é›†ï¼š%dæ¡", workerID, nickName, currentCount)
			bloggersTmp = append(bloggersTmp, blogger)
			currentCount++
		}

		// å¦‚æœæœ‰å¾…æ’å…¥çš„è®°å½•ï¼Œåˆ™æ‰¹é‡æ’å…¥
		if len(bloggersTmp) > 0 {
			// æ‰¹é‡æ’å…¥ï¼Œå‡è®¾ DB.BatchCreateBlogger å®ç°äº†æ‰¹é‡æ’å…¥å¹¶è‡ªåŠ¨å¿½ç•¥é‡å¤é”™è¯¯
			if err := DB.BatchCreateBlogger(config.GormDb, bloggersTmp); err != nil {
				log.Printf("[Worker %d] æ‰¹é‡æ’å…¥å¤±è´¥: %v", workerID, err)
			} else {
				currentCount += len(bloggersTmp)
				log.Printf("[Worker %d] æ‰¹é‡é‡‡é›†åˆ° %d ä¸ªç”¨æˆ·ï¼Œç´¯è®¡ %d æ¡", workerID, len(bloggersTmp), currentCount)
			}
		}

		// å¦‚æœå·²ç»æ²¡æœ‰ä¸‹ä¸€é¡µï¼Œåˆ™æ›´æ–°çŠ¶æ€å¹¶å¯åŠ¨æ–°ä»»åŠ¡åé€€å‡º
		if !hasNextPage {

			if err := DB.UpdateLocationState(config.GormDb, locationId, 1); err != nil {
				fmt.Println("çŠ¶æ€æ›´æ–°å¤±è´¥:", err)
			}
			log.Printf("[Worker %d] é‡‡é›†å®Œæˆ", workerID)

			return nil

		}
		// æ¯è½®é‡‡é›†åç¨ä½œç­‰å¾…
		//rand.Seed(time.Now().UnixNano())
		waitTime := time.Duration(rand.Intn(8)+3) * time.Second
		log.Printf("[Worker %d] ç­‰å¾… %s åç»§ç»­é‡‡é›†", workerID, waitTime)
		time.Sleep(waitTime)

	}

}

// SearchBloggerWorkAPi å¯åŠ¨å¤šä¸ªå¹¶å‘ä»»åŠ¡é‡‡é›†ä¸åŒåœ°ç‚¹çš„åšä¸»æ•°æ®ï¼Œä¿æŒæµè§ˆå™¨å®ä¾‹æ•°å§‹ç»ˆä¸ºæŒ‡å®šçš„ num
func SearchBloggerWorkAPi(num int) {
	var wg sync.WaitGroup

	for i := 0; i < num; i++ {
		wg.Add(1)
		go func(workerID int) {
			defer wg.Done()

			maxRetries := 5
			retryCount := 0
			for retryCount < maxRetries {
				// ä»æ•°æ®åº“è·å–ä¸€ä¸ªæœªé‡‡é›†çš„åœ°ç‚¹ID
				locations, err := DB.GetRandomUncollectedLocations(config.GormDb, 1)

				if err != nil || len(locations) == 0 {
					retryCount++
					time.Sleep(5 * time.Second)
					continue
				}

				if err != nil {
					log.Printf("[Worker %d] æŸ¥è¯¢åœ°ç‚¹å¤±è´¥: %v", workerID, err)
					return
				}
				if len(locations) == 0 {
					log.Printf("[Worker %d] æ²¡æœ‰å¯ç”¨åœ°ç‚¹ID", workerID)
					return
				}
				locationId := locations[0].LocationID // å®‰å…¨è®¿é—®

				if _, loaded := SearchLocationsMap.LoadOrStore(locationId, struct{}{}); loaded {
					log.Printf("[Worker %d] åœ°ç‚¹ %s å·²è¢«å…¶ä»–åç¨‹å¤„ç†ï¼Œè·³è¿‡", workerID, locationId)
					time.Sleep(1 * time.Second)
					continue
				}

				SearchLocationsMap.Store(locationId, struct{}{})

				// è·å–è´¦å·ï¼ˆå¤±è´¥åé‡Šæ”¾åœ°ç‚¹é”ï¼‰
				account, err := utils.GetLoginAccount("./cookies")
				if err != nil || account == "" {
					log.Printf("[Worker %d] æ— å¯ç”¨è´¦å·ï¼Œé‡Šæ”¾åœ°ç‚¹é”", workerID)
					SearchLocationsMap.Delete(locationId) // å…³é”®ä¿®æ”¹ï¼šç«‹å³é‡Šæ”¾é”
					if err := AddNewAccount(workerID); err != nil {
						log.Printf("[Worker %d] è¡¥å·å¤±è´¥: %v", workerID, err)
					}
					continue
				}
				// è°ƒç”¨ SearchJobApi è¿›è¡Œé‡‡é›†ï¼ˆå†…éƒ¨ä¼šåˆ›å»ºæ–°çš„æµè§ˆå™¨å®ä¾‹ï¼Œå¹¶é€šè¿‡ defer å…³é—­ï¼‰
				log.Printf("[Worker %d] å¯åŠ¨æ–°æµè§ˆå™¨å®ä¾‹ï¼Œå¤„ç†åœ°ç‚¹: %s", workerID, locationId)
				err = SearchJobApi(workerID, account, locationId)
				if err != nil {
					log.Printf("[Worker %d] é‡‡é›†ä»»åŠ¡å¤±è´¥ï¼ˆ%sï¼‰ï¼Œå…³é—­å½“å‰æµè§ˆå™¨ï¼Œé‡æ–°è·å–æ–°åœ°ç‚¹", workerID, err)
					// å¼ºåˆ¶é‡Šæ”¾åœ°ç‚¹é”ï¼ˆå…è®¸å…¶ä»–åç¨‹é‡è¯•ï¼‰
					SearchLocationsMap.Delete(locationId) // å…³é”®ä¿®æ”¹ï¼šå¤±è´¥æ—¶åˆ é™¤é”

				} else {
					log.Printf("[Worker %d] é‡‡é›†æˆåŠŸï¼Œå‡†å¤‡ä¸‹ä¸€ä»»åŠ¡", workerID)
				}
				// ç¨ä½œå»¶æ—¶ï¼Œé˜²æ­¢è¿‡å¿«è½®è¯¢
				// ä»»åŠ¡é—´éš”æ§åˆ¶
				interval := config.AppCfg.CrawlersTimeInterval
				if interval == 0 {
					interval = 5
				}
				wait := time.Duration(rand.Intn(interval)) * time.Minute
				log.Printf("[Worker %d] ç­‰å¾… %v åç»§ç»­", workerID, wait)
				time.Sleep(wait)

				retryCount = 0 // é‡ç½®é‡è¯•è®¡æ•°å™¨
			}

			if retryCount >= maxRetries {
				log.Printf("[Worker %d] è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œé€€å‡ºä»»åŠ¡", workerID)
				return
			}
		}(i)
	}
	wg.Wait()
	log.Println("æ‰€æœ‰æµè§ˆå™¨å®ä¾‹æ‰§è¡Œå®Œæ¯•")
}
